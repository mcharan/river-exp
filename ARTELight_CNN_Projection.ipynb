{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997688a5-c6a9-4a04-ab36-6f464e5089f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import statistics\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import psutil\n",
    "from river import base, stats, utils, drift, metrics, preprocessing, datasets\n",
    "from river.datasets import synth\n",
    "from deep_river import classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7de35ea-8e88-4f90-a51c-520ef4da6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. FUNÇÕES DE SUPORTE ---\n",
    "def generate_rotation_matrix(n_features):\n",
    "    random_matrix = torch.randn(n_features, n_features)\n",
    "    q, _ = torch.linalg.qr(random_matrix)\n",
    "    return q\n",
    "\n",
    "def log_results_to_csv(filename, data_dict):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, mode='a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=data_dict.keys())\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59a1374-ccf4-4eae-ad25-f14753d17078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. CONFIGURAÇÃO DE DATASETS  ---\n",
    "def get_dataset(name, seed, n_classes=2):\n",
    "    \"\"\"Retorna o stream e as dimensões esperadas pós-encoding.\"\"\"\n",
    "    if name.lower() == 'elec2':\n",
    "        # Elec2 tem 8 features originais -> ~17 após OneHot das categorias\n",
    "        return datasets.Elec2(), 17, 2\n",
    "    elif name.lower() == 'agrawal':\n",
    "        return synth.Agrawal(seed=seed), 10, 2\n",
    "    elif name.lower() == 'sea':\n",
    "        return synth.SEA(seed=seed), 3, 2\n",
    "    elif name.lower() == 'sine':\n",
    "        return synth.Sine(seed=seed), 3, 2\n",
    "    elif name.lower() == 'hyperplane':\n",
    "        return synth.Hyperplane(seed=seed, n_features=10), 10, 2\n",
    "    elif name.lower() == 'randomtree':\n",
    "        # Usando os parâmetros específicos fornecidos\n",
    "        return synth.RandomTree(\n",
    "            seed_tree=seed, \n",
    "            seed_sample=seed, \n",
    "            n_classes=n_classes, \n",
    "            n_num_features=5, \n",
    "            n_cat_features=5\n",
    "        ), 15, n_classes # 5 num + 5 cat (que expandem no OHE)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset {name} não configurado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1603c2-c169-4802-ba18-9dd13a65f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. CLASSE DAS REDES NEURAIS (Mix Heterogêneo) ---\n",
    "class FlexibleNeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, hidden_layers=[32], use_cnn=False, projection_matrix=None):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.use_cnn = use_cnn\n",
    "        if projection_matrix is not None:\n",
    "            self.register_buffer('projection', projection_matrix.to(torch.float32))\n",
    "        else:\n",
    "            self.projection = None\n",
    "        self.in_dim = (8 * n_features) if use_cnn else n_features\n",
    "        if use_cnn:\n",
    "            self.cnn_block = nn.Sequential(nn.Conv1d(1, 8, 3, padding=1), nn.ReLU(), nn.Flatten())\n",
    "        else:\n",
    "            self.cnn_block = None\n",
    "        layers = []\n",
    "        curr = self.in_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(curr, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            curr = h\n",
    "        layers.append(nn.Linear(curr, n_classes))\n",
    "        self.mlp_head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1: x = x.unsqueeze(0)\n",
    "        x = x.to(torch.float32)\n",
    "        if x.shape[1] != self.n_features:\n",
    "            tmp = torch.zeros((x.shape[0], self.n_features), device=x.device)\n",
    "            tmp[:, :min(x.shape[1], self.n_features)] = x[:, :min(x.shape[1], self.n_features)]\n",
    "            x = tmp\n",
    "        if self.projection is not None: x = torch.matmul(x, self.projection)\n",
    "        if self.use_cnn: x = self.cnn_block(x.unsqueeze(1))\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268a5229-f553-455a-89a4-719c8377a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleNN(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, hidden_layers=[10]):\n",
    "        super(FlexibleNN, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = n_features\n",
    "        \n",
    "        for h_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_dim, n_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ba5c00a-ac94-4fa5-a7d6-98d430262875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARTELight(base.Ensemble, base.Classifier):\n",
    "    def __init__(self, models, drift_detector, lambda_val=10.0, seed=42):\n",
    "        super().__init__(models=models)\n",
    "        self.lambda_val = lambda_val\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        self.drift_detector = drift_detector\n",
    "        self._detectors = [drift_detector.clone() for _ in range(len(models))]\n",
    "        self._acc_windows = [utils.Rolling(stats.Mean(), window_size=100) for _ in range(len(models))]\n",
    "        self._total_drifts = 0\n",
    "\n",
    "    def learn_one(self, x, y):\n",
    "        any_drift = False # Variável para controlar se houve drift neste passo\n",
    "        for i, model in enumerate(self.models):\n",
    "            y_pred = model.predict_one(x)\n",
    "            correct = (y == y_pred)\n",
    "            \n",
    "            self._detectors[i].update(0 if correct else 1)\n",
    "            self._acc_windows[i].update(1 if correct else 0)\n",
    "            \n",
    "            if not correct:\n",
    "                k = self._rng.poisson(self.lambda_val)\n",
    "                for _ in range(k):\n",
    "                    model.learn_one(x, y)\n",
    "\n",
    "            if self._detectors[i].drift_detected:\n",
    "                self._total_drifts += 1\n",
    "                self.models[i] = model.clone()\n",
    "                self._detectors[i] = self.drift_detector.clone()\n",
    "                self._acc_windows[i] = utils.Rolling(stats.Mean(), window_size=100)\n",
    "                any_drift = True # Sinaliza que houve ao menos um drift\n",
    "                \n",
    "        return any_drift # Retorna para o main() decidir sobre o gc.collect()\n",
    "\n",
    "    def predict_proba_one(self, x):\n",
    "        accs = [w.get() for w in self._acc_windows]\n",
    "        avg = sum(accs)/len(accs) if accs else 0\n",
    "        idx = [i for i, a in enumerate(accs) if a >= avg] or range(len(self.models))\n",
    "        \n",
    "        votes = collections.Counter()\n",
    "        for i in idx:\n",
    "            p = self.models[i].predict_proba_one(x)\n",
    "            for c, v in p.items(): \n",
    "                votes[c] += v / len(idx)\n",
    "        return votes\n",
    "\n",
    "    @property\n",
    "    def total_drifts(self): \n",
    "        return self._total_drifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13f5c6e1-71a3-4e6d-a309-289a61b0bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- WRAPPER PARA DESBALANCEAMENTO (Replicando BinaryImbalancedGenerator.java) ---\n",
    "class ImbalancedStream:\n",
    "    def __init__(self, stream, ir=0.1, seed=42):\n",
    "        self.stream = stream\n",
    "        self.ir = ir # Ex: 0.05 para 5% de classe minoritária\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.iterator = iter(self.stream)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # Decide qual classe queremos com base na probabilidade IR\n",
    "        target_class = 1 if self.rng.rand() < self.ir else 0\n",
    "        \n",
    "        # Consome o stream original até achar a classe desejada\n",
    "        while True:\n",
    "            try:\n",
    "                x, y = next(self.iterator)\n",
    "                y_val = int(y) if isinstance(y, (int, float, bool, np.bool_)) else int(y)\n",
    "                if y_val == target_class:\n",
    "                    return x, y_val\n",
    "            except StopIteration:\n",
    "                raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f484ac44-dce9-4d20-b3b3-cd74f181c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. EXECUÇÃO DO EXPERIMENTO ---\n",
    "def main(dataset='elec2', ir=None, seed=123456789, exp_type='binary', n_models=30, n_classes=2, batch_size=32):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 1. Obter Stream e Configuração (respeitando n_classes para sintéticos)\n",
    "    stream_raw, n_feat, ds_classes = get_dataset(dataset, seed, n_classes)\n",
    "    # Se o dataset real tiver número fixo de classes (como Elec2), usamos o do dataset\n",
    "    final_n_classes = ds_classes if dataset.lower() != 'randomtree' else n_classes\n",
    "    \n",
    "    # 2. Aplicar Wrapper de Desbalanceamento (Protocolo Alberto Cano)\n",
    "    # Se ir=None, o stream segue original (ex: Elec2)\n",
    "    if ir is not None:\n",
    "        print(f\"Aplicando Static Imbalance Ratio: {ir*100}%\")\n",
    "        stream = ImbalancedStream(stream_raw, ir=ir, seed=seed)\n",
    "    else:\n",
    "        stream = stream_raw\n",
    "\n",
    "    loss_f = nn.CrossEntropyLoss() # Definida uma única vez para todos\n",
    "    # Mix de Arquiteturas Heterogêneas\n",
    "    ensemble_list = []\n",
    "    torch.manual_seed(seed)\n",
    "    for i in range(n_models):\n",
    "        if i < n_models//3:\n",
    "            cfg = {\"cnn\": False, \"opt\": optim.SGD, \"lr\": 0.05, \"layers\": [128, 64], \"proj\": False}\n",
    "        elif i < 2*n_models//3:\n",
    "            cfg = {\"cnn\": True, \"opt\": optim.Adam, \"lr\": 0.01, \"layers\": [64], \"proj\": False}\n",
    "        else:\n",
    "            cfg = {\"cnn\": False, \"opt\": optim.Adam, \"lr\": 0.005, \"layers\": [256, 128], \"proj\": True}\n",
    "        \n",
    "        proj = torch.randn(n_feat, n_feat) if cfg[\"proj\"] else None\n",
    "        if proj is not None: proj, _ = torch.linalg.qr(proj)\n",
    "\n",
    "        m = classification.Classifier(\n",
    "            module=FlexibleNeuralNetwork(n_feat, final_n_classes, cfg[\"layers\"], cfg[\"cnn\"], proj),\n",
    "            loss_fn=loss_f, \n",
    "            optimizer_fn=cfg[\"opt\"], \n",
    "            lr=cfg[\"lr\"], \n",
    "            device=device, \n",
    "            is_feature_incremental=False\n",
    "        )\n",
    "        ensemble_list.append(m)\n",
    "\n",
    "    model_artelmb = ARTELight(ensemble_list, drift.ADWIN(delta=1e-3), lambda_val=10.0, seed=seed)\n",
    "    # model_artelmb = ARTELightMB(ensemble_list, drift.ADWIN(delta=0.002), lambda_val=1.0, batch_limit=32, seed=seed)\n",
    "    \n",
    "    # Métricas do Survey [cite: 415, 469]\n",
    "    metric_acc = metrics.Accuracy()\n",
    "    metric_kappa = metrics.CohenKappa()\n",
    "    metric_gmean = metrics.GeometricMean()\n",
    "    \n",
    "    oh_encoder = preprocessing.OneHotEncoder()\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_batch, y_batch, latencies = [], [], []\n",
    "    # Organização de arquivos seguindo o padrão do repositório\n",
    "    os.makedirs(f\"results/{exp_type}\", exist_ok=True)\n",
    "    ir_str = f\"_ir{ir}\" if ir else \"\"\n",
    "    output_file = f\"results/{exp_type}/{dataset}{ir_str}_s{seed}_m{n_models}.csv\"\n",
    "    \n",
    "    print(f\"Executando: {dataset.upper()} | Tipo: {exp_type} | Seed: {seed} | N_Feat: {n_feat}\")\n",
    "\n",
    "    for count, (x, y) in enumerate(stream, 1):\n",
    "        # Conversão de label (funciona para binário e multiclasse)\n",
    "        y_val = int(y) if isinstance(y, (int, float, bool)) else y\n",
    "        X_batch.append(x); y_batch.append(1 if y else 0)\n",
    "        \n",
    "        if len(X_batch) == batch_size:\n",
    "            start = time.perf_counter()\n",
    "\n",
    "            # Preprocessing\n",
    "            X_df = pd.DataFrame(X_batch)\n",
    "            oh_encoder.learn_many(X_df)\n",
    "            X_oh = oh_encoder.transform_many(X_df).astype(np.float64)\n",
    "            if hasattr(X_oh, \"sparse\"): X_oh = X_oh.sparse.to_dense()\n",
    "            scaler.learn_many(X_oh)\n",
    "            X_sc = scaler.transform_many(X_oh)\n",
    "            \n",
    "            # Padding dinâmico baseado no n_feat da configuração\n",
    "            data_fix = np.zeros((batch_size, n_feat))\n",
    "            cols = min(X_sc.shape[1], n_feat)\n",
    "            data_fix[:, :cols] = X_sc.iloc[:, :cols].values\n",
    "            X_dicts = pd.DataFrame(data_fix, columns=[f\"f{j}\" for j in range(n_feat)]).to_dict(orient='records')\n",
    "\n",
    "            # Test-then-Train\n",
    "            any_drift = False\n",
    "            for i in range(batch_size):\n",
    "                y_pred = model_artelmb.predict_one(X_dicts[i])\n",
    "                if y_pred is not None:\n",
    "                    metric_acc.update(y_batch[i], y_pred)\n",
    "                    metric_kappa.update(y_batch[i], y_pred)\n",
    "                    metric_gmean.update(y_batch[i], y_pred)\n",
    "                # O learn_one agora retorna True se houver drift\n",
    "                if model_artelmb.learn_one(X_dicts[i], y_batch[i]):\n",
    "                    any_drift = True\n",
    "\n",
    "            # Limpeza de memória fora do loop de instâncias, apenas se necessário\n",
    "            if any_drift:\n",
    "                import gc\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            latencies.append((time.perf_counter() - start) * 1000)\n",
    "            X_batch, y_batch = [], []\n",
    "\n",
    "            if count % 2000 < batch_size:\n",
    "                ram = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "                vram = torch.cuda.memory_allocated() / (1024*1024) if torch.cuda.is_available() else 0\n",
    "                avg_lat = sum(latencies[-20:])/20 / batch_size\n",
    "                \n",
    "                stats_dict = {\n",
    "                    \"Instancia\": count, \n",
    "                    \"Dataset\": dataset, \n",
    "                    \"Seed\": seed,\n",
    "                    \"Accuracy\": metric_acc.get(), \n",
    "                    \"Kappa\": metric_kappa.get(),\n",
    "                    \"GMean\": metric_gmean.get(),\n",
    "                    \"Latencia_ms\": avg_lat, \n",
    "                    \"Drifts\": model_artelmb.total_drifts,\n",
    "                    \"RAM_MB\": ram, \n",
    "                    \"VRAM_MB\": vram\n",
    "                }\n",
    "                log_results_to_csv(output_file, stats_dict)\n",
    "                print(f\"Inst: {count} | Acc: {stats_dict['Accuracy']:.2%} | Kappa: {stats_dict['Kappa']:.2f} | Drifts: {stats_dict['Drifts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc69662c-3b9d-44f7-b595-89b66812f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiente Notebook detectado. Usando parâmetros padrão...\n",
      "Executando: ELEC2 | Tipo: binary | Seed: 123456789 | N_Feat: 17\n",
      "Inst: 2016 | Acc: 79.81% | Kappa: 0.58 | Drifts: 58\n",
      "Inst: 4000 | Acc: 78.67% | Kappa: 0.55 | Drifts: 71\n",
      "Inst: 6016 | Acc: 78.56% | Kappa: 0.55 | Drifts: 73\n",
      "Inst: 8000 | Acc: 78.11% | Kappa: 0.55 | Drifts: 99\n",
      "Inst: 10016 | Acc: 78.36% | Kappa: 0.56 | Drifts: 112\n",
      "Inst: 12000 | Acc: 78.55% | Kappa: 0.56 | Drifts: 122\n",
      "Inst: 14016 | Acc: 79.11% | Kappa: 0.58 | Drifts: 150\n",
      "Inst: 16000 | Acc: 79.25% | Kappa: 0.58 | Drifts: 165\n",
      "Inst: 18016 | Acc: 79.42% | Kappa: 0.58 | Drifts: 172\n",
      "Inst: 20000 | Acc: 79.88% | Kappa: 0.59 | Drifts: 179\n",
      "Inst: 22016 | Acc: 79.91% | Kappa: 0.59 | Drifts: 182\n",
      "Inst: 24000 | Acc: 80.00% | Kappa: 0.59 | Drifts: 190\n",
      "Inst: 26016 | Acc: 80.35% | Kappa: 0.60 | Drifts: 202\n",
      "Inst: 28000 | Acc: 80.62% | Kappa: 0.60 | Drifts: 237\n",
      "Inst: 30016 | Acc: 80.89% | Kappa: 0.61 | Drifts: 247\n",
      "Inst: 32000 | Acc: 80.92% | Kappa: 0.61 | Drifts: 265\n",
      "Inst: 34016 | Acc: 80.98% | Kappa: 0.61 | Drifts: 284\n",
      "Inst: 36000 | Acc: 80.99% | Kappa: 0.61 | Drifts: 289\n",
      "Inst: 38016 | Acc: 80.98% | Kappa: 0.61 | Drifts: 304\n",
      "Inst: 40000 | Acc: 81.01% | Kappa: 0.61 | Drifts: 317\n",
      "Inst: 42016 | Acc: 80.91% | Kappa: 0.61 | Drifts: 320\n",
      "Inst: 44000 | Acc: 80.91% | Kappa: 0.61 | Drifts: 340\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Verifica se estamos no Jupyter ou Terminal\n",
    "    # O Jupyter geralmente tem 'ipykernel_launcher' ou '-f' nos argumentos\n",
    "    is_jupyter = any('ipykernel' in arg or '-f' in arg for arg in sys.argv)\n",
    "    \n",
    "    # Se rodado via linha de comando\n",
    "    if not is_jupyter and len(sys.argv) > 1:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--dataset', type=str, default='elec2')\n",
    "        parser.add_argument('--seed', type=int, default=123456789)\n",
    "        parser.add_argument('--batch_size', type=int, default=32)\n",
    "        parser.add_argument('--exp_type', type=str, default='real_world')\n",
    "        parser.add_argument('--n_models', type=int, default=30)\n",
    "        parser.add_argument('--n_classes', type=int, default=2)\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        main(dataset=args.dataset, seed=args.seed, batch_size=args.batch_size, \n",
    "             n_models=args.n_models, n_classes=args.n_classes)\n",
    "    else:\n",
    "        # Execução via Notebook (parâmetros manuais abaixo)\n",
    "        print(\"Ambiente Notebook detectado. Usando parâmetros padrão...\")\n",
    "        # main(dataset='agrawal', ir=0.1, seed=123456789, exp_type='static_ir')\n",
    "        # main(dataset='agrawal', ir=0.01, seed=123456789, exp_type='static_ir')\n",
    "        main(\n",
    "            dataset='elec2', \n",
    "            seed=123456789, \n",
    "            exp_type='binary', \n",
    "            batch_size=32,\n",
    "            n_models=30, n_classes=2\n",
    "        )\n",
    "        \n",
    "        # main(dataset='elec2', seed=123456789, exp_type='binary', batch_size=32, n_models=30, n_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21db2a8-2a6b-4b76-b1d1-7cc7061956fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
