{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0904cc-1d54-42b4-845f-871b076516f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from deep_river import classification\n",
    "from river import metrics, ensemble, stream, preprocessing, compose\n",
    "import time\n",
    "import os\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075ba30f-3f17-4723-93fc-40eb1b1d02ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo detectado: cuda\n",
      "GPU em uso: Quadro P620\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define o dispositivo globalmente\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo detectado: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU em uso: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd0ba3-4ecd-4ee2-b070-407d955317b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando PoC: ADWINBagging + 10 Redes Neurais (Deep River)\n",
      "Dataset: ElecNorm | Algoritmo: Online Bagging com ADWIN\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Arquitetura Shallow\n",
    "class ShallowNN(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(ShallowNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# 2. Configurações de Dados (Elec2)\n",
    "base_path = os.path.expanduser(\"~/moa/aldopaim/AdaptiveRegularizedEnsemble/datasets\")\n",
    "filename = \"elecNormNew.arff\"\n",
    "file_path = os.path.join(base_path, filename)\n",
    "target_column = \"class\"\n",
    "\n",
    "# Dimensões iniciais para Elec2\n",
    "n_features = 8 \n",
    "n_classes = 2\n",
    "\n",
    "# 3. Classificador Base Deep River (Corrigido conforme seu feedback)\n",
    "base_nn = classification.Classifier(\n",
    "    module=ShallowNN(n_features=n_features, n_classes=n_classes),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer_fn=optim.SGD,\n",
    "    lr=0.5, # Aumentado para convergir mais rápido\n",
    "    is_feature_incremental=True,\n",
    "    is_class_incremental=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 4. Ensemble ADWINBagging\n",
    "# Mais simples que o ARTE, ideal para validar o aprendizado base\n",
    "model = ensemble.ADWINBaggingClassifier(\n",
    "    model=base_nn,\n",
    "    n_models=50, # Aumentado de 5 para 10 para melhor robustez\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 5. Pipeline de Normalização e Tratamento Categórico\n",
    "pipeline = compose.Pipeline(\n",
    "    preprocessing.OneHotEncoder(), \n",
    "    preprocessing.StandardScaler(),\n",
    "    model\n",
    ")\n",
    "\n",
    "# 6. Loop de Execução\n",
    "label_map = {}\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "print(f\"Iniciando PoC: ADWINBagging + 10 Redes Neurais (Deep River)\")\n",
    "print(f\"Dataset: ElecNorm | Algoritmo: Online Bagging com ADWIN\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "count = 0\n",
    "\n",
    "try:\n",
    "    dataset_stream = stream.iter_arff(file_path, target=target_column)\n",
    "    \n",
    "    for x, y in dataset_stream:\n",
    "        if y not in label_map:\n",
    "            label_map[y] = len(label_map)\n",
    "        y_numeric = label_map[y]\n",
    "\n",
    "        # Test-then-Train\n",
    "        y_pred = pipeline.predict_one(x)\n",
    "        if y_pred is not None:\n",
    "            metric.update(y_numeric, y_pred)\n",
    "        \n",
    "        pipeline.learn_one(x, y_numeric)\n",
    "        count += 1\n",
    "        \n",
    "        if count % 2000 == 0:\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            mem = get_memory_usage()\n",
    "            print(f\"{count:<12} | {metric.get():>9.2%} | {elapsed:>9.2f} | {mem:>12.2f}\")\n",
    "\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"FINAL: Acc: {metric.get():.2%} | Tempo: {time.perf_counter()-start_time:.2f}s\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1047f83-e8f8-456a-b9e9-54196e2ac8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e331d0-4493-4da0-9a1c-8127d64cd14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a619bfe7-5b31-4d88-840d-ac6fe4e950c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARTE(base.Ensemble, base.Classifier):\n",
    "    \"\"\"Adaptive Random Tree Ensemble (ARTE) portado do MOA.\n",
    "    \n",
    "    Algoritmo adaptativo para fluxos de dados evolutivos de Paim e Enembreck.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: base.Classifier = None,\n",
    "        n_models: int = 100,\n",
    "        lambd: float = 6.0,\n",
    "        drift_detector: base.DriftDetector = None,\n",
    "        window_size: int = 1000,\n",
    "        n_rejections: int = 5,\n",
    "        seed: int = 1\n",
    "    ):\n",
    "        # O modelo base sugerido no original é a ARFHoeffdingTree\n",
    "        # No River, usamos HoeffdingTreeClassifier como base\n",
    "        self.model = model or tree.HoeffdingTreeClassifier()\n",
    "        self.n_models = n_models\n",
    "        self.lambd = lambd\n",
    "        self.drift_detector = drift_detector or drift.ADWIN(delta=1e-3)\n",
    "        self.window_size = window_size\n",
    "        self.n_rejections = n_rejections\n",
    "        self.seed = seed\n",
    "        self._rng = np.random.RandomState(self.seed)\n",
    "        \n",
    "        # Inicialização dos membros conforme a estrutura AREBaseLearner do original\n",
    "        self._ensemble_members = []\n",
    "        for i in range(self.n_models):\n",
    "            m = {\n",
    "                'model': self.model.clone(),\n",
    "                'detector': self.drift_detector.clone(),\n",
    "                'untrained_counts': collections.defaultdict(int),\n",
    "                'window_acc': utils.Rolling(stats.Mean(), window_size=self.window_size),\n",
    "                'instances_trained': 0\n",
    "            }\n",
    "            self._ensemble_members.append(m)\n",
    "            \n",
    "        super().__init__(models=[m['model'] for m in self._ensemble_members])\n",
    "        self._avg_window_acc = 0.0\n",
    "\n",
    "    def learn_one(self, x, y):\n",
    "        all_accs = []\n",
    "        \n",
    "        for m in self._ensemble_members:\n",
    "            # Predição para controle de erro e lógica de rejeição\n",
    "            y_pred = m['model'].predict_one(x)\n",
    "            correct = (y == y_pred)\n",
    "            \n",
    "            # Estratégia de Regularização Adaptativa:\n",
    "            # Para evitar que domínios com ruído dominem, treina no erro\n",
    "            # ou após N rejeições (acertos)\n",
    "            will_train = not correct\n",
    "            \n",
    "            if correct:\n",
    "                m['untrained_counts'][y] += 1\n",
    "                if self.n_rejections > 0 and m['untrained_counts'][y] >= self.n_rejections:\n",
    "                    m['untrained_counts'][y] = 0\n",
    "                    will_train = True\n",
    "            \n",
    "            if will_train:\n",
    "                # Online Bagging via Poisson\n",
    "                k = self._rng.poisson(self.lambd)\n",
    "                if k > 0:\n",
    "                    for _ in range(k):\n",
    "                        m['model'].learn_one(x, y)\n",
    "                        m['instances_trained'] += 1\n",
    "            \n",
    "            # Detecção de Drift individual\n",
    "            m['detector'].update(0 if correct else 1)\n",
    "            if m['detector'].drift_detected:\n",
    "                self._reset_member(m)\n",
    "            \n",
    "            # Atualiza estatísticas da janela deslizante\n",
    "            m['window_acc'].update(1 if correct else 0)\n",
    "            all_accs.append(m['window_acc'].get())\n",
    "\n",
    "        # Atualiza média global para critério de votação seletiva\n",
    "        if all_accs:\n",
    "            self._avg_window_acc = statistics.mean(all_accs)\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def predict_proba_one(self, x):\n",
    "        combined_votes = collections.Counter()\n",
    "        \n",
    "        # O ARTE filtra votantes cuja acurácia na janela é inferior à média global\n",
    "        eligible_members = [\n",
    "            m for m in self._ensemble_members \n",
    "            if self.window_size == 0 or m['window_acc'].get() >= self._avg_window_acc\n",
    "        ]\n",
    "        \n",
    "        # Fallback se ninguém estiver acima da média (ex: início do stream)\n",
    "        if not eligible_members:\n",
    "            eligible_members = self._ensemble_members\n",
    "\n",
    "        for m in eligible_members:\n",
    "            votes = m['model'].predict_proba_one(x)\n",
    "            if votes:\n",
    "                total = sum(votes.values())\n",
    "                if total > 0:\n",
    "                    for cls, prob in votes.items():\n",
    "                        combined_votes[cls] += prob / total\n",
    "\n",
    "        return combined_votes\n",
    "\n",
    "    def _reset_member(self, m):\n",
    "        \"\"\"Reinicia o modelo e estatísticas após detecção de mudança.\"\"\"\n",
    "        m['model'] = self.model.clone()\n",
    "        m['detector'] = self.drift_detector.clone()\n",
    "        m['untrained_counts'].clear()\n",
    "        m['window_acc'] = utils.Rolling(stats.Mean(), window_size=self.window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c05000f8-9b74-4b6a-a949-85f86e874c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando ARTE + Deep River (Redes Neurais)\n",
      "Pipeline: OneHot -> Scaler -> Ensemble\n",
      "-----------------------------------------------------------------\n",
      "1000         |    78.70% |     13.25 |       581.16\n",
      "2000         |    78.70% |     35.93 |       581.91\n",
      "3000         |    77.47% |     67.12 |       583.41\n",
      "4000         |    77.15% |    105.05 |       584.79\n",
      "5000         |    76.62% |    150.67 |       586.04\n",
      "6000         |    76.57% |    199.82 |       586.54\n",
      "7000         |    76.30% |    255.66 |       588.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m     metric\u001b[38;5;241m.\u001b[39mupdate(y_numeric, y_pred)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# O pipeline processa x e o modelo treina com y_numeric\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-river-demo/lib/python3.10/site-packages/river/compose/pipeline.py:474\u001b[0m, in \u001b[0;36mPipeline.learn_one\u001b[0;34m(self, x, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# Here the step is not a transformer, and it's supervised, such as a LinearRegression.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# This is usually the last step of the pipeline.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m step\u001b[38;5;241m.\u001b[39m_supervised:\n\u001b[0;32m--> 474\u001b[0m     \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# Here the step is not a transformer, and it's unsupervised, such as a KMeans. This\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# is also usually the last step of the pipeline.\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     step\u001b[38;5;241m.\u001b[39mlearn_one(x\u001b[38;5;241m=\u001b[39mx)\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mARTE.learn_one\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[0;32m---> 67\u001b[0m             \u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m             m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstances_trained\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Detecção de Drift individual\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-river-demo/lib/python3.10/site-packages/deep_river/classification/classifier.py:155\u001b[0m, in \u001b[0;36mClassifier.learn_one\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: \u001b[38;5;28mdict\u001b[39m, y: base\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mClfTarget) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn from a single instance.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m        Class label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_observed_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_observed_targets(y)\n\u001b[1;32m    157\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dict2tensor(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-river-demo/lib/python3.10/site-packages/deep_river/base.py:210\u001b[0m, in \u001b[0;36mDeepEstimator._update_observed_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m prev_feature_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserved_features)\n\u001b[1;32m    209\u001b[0m new_features \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobserved_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_feature_incremental\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_input_size() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserved_features)\n\u001b[1;32m    215\u001b[0m ):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_layer(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer, target_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserved_features), output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-river-demo/lib/python3.10/site-packages/sortedcontainers/sortedset.py:687\u001b[0m, in \u001b[0;36mSortedSet.update\u001b[0;34m(self, *iterables)\u001b[0m\n\u001b[1;32m    685\u001b[0m     _set\u001b[38;5;241m.\u001b[39mupdate(values)\n\u001b[1;32m    686\u001b[0m     _list\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 687\u001b[0m     \u001b[43m_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     _add \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add\n",
      "File \u001b[0;32m~/anaconda3/envs/deep-river-demo/lib/python3.10/site-packages/sortedcontainers/sortedlist.py:338\u001b[0m, in \u001b[0;36mSortedList.update\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    336\u001b[0m _lists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lists\n\u001b[1;32m    337\u001b[0m _maxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maxes\n\u001b[0;32m--> 338\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _maxes:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_len:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from deep_river import classification\n",
    "from river import metrics, drift, stream, preprocessing, compose\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "# 1. Definição da Arquitetura da Rede Neural (PyTorch)\n",
    "class ShallowNN(nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(ShallowNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# 2. Configurações de Dados\n",
    "base_path = os.path.expanduser(\"~/moa/aldopaim/AdaptiveRegularizedEnsemble/datasets\")\n",
    "filename = \"elecNormNew.arff\"\n",
    "file_path = os.path.join(base_path, filename)\n",
    "target_column = \"class\"\n",
    "\n",
    "# Nota: O número de features pode mudar após o OneHotEncoder. \n",
    "# Para o Elec2 com OneHot no 'day', passamos de 8 para 14 features.\n",
    "n_features = 8 \n",
    "n_classes = 2\n",
    "\n",
    "# 3. Construção do Classificador Base (Deep River)\n",
    "base_nn = classification.Classifier(\n",
    "    module=ShallowNN(n_features=n_features, n_classes=n_classes),  # Passamos a classe da rede\n",
    "    module_params={\"n_classes\": 2}, # O wrapper injetará n_features\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    optimizer_fn=optim.SGD,\n",
    "    lr=0.05,\n",
    "    # Habilita a expansão dinâmica de entradas e saídas\n",
    "    is_feature_incremental=True, \n",
    "    is_class_incremental=True\n",
    ")\n",
    "# base_nn = classification.Classifier(\n",
    "#     module=ShallowNN(n_features=n_features, n_classes=n_classes),\n",
    "#     loss_fn=nn.CrossEntropyLoss(),\n",
    "#     optimizer_fn=optim.SGD,\n",
    "#     lr=0.05\n",
    "# )\n",
    "\n",
    "# 4. Criação do Ensemble ARTE\n",
    "model = ARTE(\n",
    "    model=base_nn,\n",
    "    n_models=5,\n",
    "    drift_detector=drift.ADWIN(delta=1e-3),\n",
    "    window_size=500,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 5. Pipeline Robusto:\n",
    "# - OneHotEncoder: transforma strings em 0/1 (resolve o erro do scaler)\n",
    "# - StandardScaler: normaliza para a rede neural\n",
    "pipeline = compose.Pipeline(\n",
    "    preprocessing.OneHotEncoder(), \n",
    "    preprocessing.StandardScaler(),\n",
    "    model\n",
    ")\n",
    "\n",
    "# 6. Loop de Execução com Mapeamento de Classes\n",
    "label_map = {}\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "print(f\"Iniciando ARTE + Deep River (Redes Neurais)\")\n",
    "print(f\"Pipeline: OneHot -> Scaler -> Ensemble\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "count = 0\n",
    "\n",
    "try:\n",
    "    dataset_stream = stream.iter_arff(file_path, target=target_column)\n",
    "    \n",
    "    for x, y in dataset_stream:\n",
    "        # Mapeia rótulos string para numérico para o CrossEntropy do PyTorch\n",
    "        if y not in label_map:\n",
    "            label_map[y] = len(label_map)\n",
    "        y_numeric = label_map[y]\n",
    "\n",
    "        # Prequencial: Teste-antes-do-Treino\n",
    "        y_pred = pipeline.predict_one(x)\n",
    "        if y_pred is not None:\n",
    "            metric.update(y_numeric, y_pred)\n",
    "        \n",
    "        # O pipeline processa x e o modelo treina com y_numeric\n",
    "        pipeline.learn_one(x, y_numeric)\n",
    "        count += 1\n",
    "        \n",
    "        if count % 1000 == 0:\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            mem = get_memory_usage()\n",
    "            print(f\"{count:<12} | {metric.get():>9.2%} | {elapsed:>9.2f} | {mem:>12.2f}\")\n",
    "\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"SUCESSO: Acc Final: {metric.get():.2%} | Rótulos: {label_map}\")\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(f\"\\nErro durante a execução: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebdc736-5412-4381-8e49-c73e63b53310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848acdb0-9489-4dc9-a99f-cc63ba42c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f2248-57ac-493d-9bbf-3b3ac1c62d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
